# Casos prácticos {#caos}


DIEGO, EN EL CSS SE PUEDE AÑADIR UN CUADRO QUE SEA UNA ADVERTENCIA, YO NO SÉ,
lO PROBAMOS/PRUEBAS :) O NO??



<!-- ::: {.infobox .caution data-latex="{caution}"} -->
<!-- **NOTA** -->

<!-- En las siguientes aplicaciones se asumen que los datos se localizan -->
<!-- en la carpeta `data` del proyecto de R-Studio en el que se esté trabajando. -->
<!-- ::: -->





## Caso 1: Temperatura mínimas del aire en España. 

**Objetivo de aprendizaje**

Esta sección presenta un caso de uso en el que aprenderemos a realizar las
siguientes tareas básicas:

1. Peer datos espaciales en R.

2. Proyectar datos espaciales.

3. Graficar datos espaciales.


Para ello, se va a trabajar con los datos de temperatura mínima registradas en España
por las estaciones metereológicas de la Agencia Estatal de Meteorología (AEMET).
Un conjuto de datos ya depurados y listos par trabajar se encuentra en el fichero `tempmin.csv`.
Por otra parte, la inforación relativa al mapeo de España se obtendrá directamente
de la librería `mapSpain`.
Todo el análisis se va a realizar empleando RStudio, por lo que se empezará
abriendo el programa y creando un nuevo proyecto.


::: {.exercise #ex-crea name="Creación del proyecto" }

Cree un proyecto para trabajar todo lo referente al caso.
:::

::: {.solution #sol-crea}
Para crear un proyecto siga la secuencia:
*File> New Proyect> New File> RMD*
:::



El conjunto de datos proporcionado `tempmin.csv` contiene el nivel de
temperatura del aire en España entre el 6 y el 10 de Enero de
2021[^ap1_aplic_tmin-1]. Estos datos han sido descargados usando la librería
`climaemet` [@R-climaemet] y han sido posteriormente tratados para su uso en
esta práctica.

[^ap1_aplic_tmin-1]: Las fechas seleccionadas coinciden con el periodo en el que
    la tormenta Filomena tuvo su auge en la Península Ibérica.

El primer paso consiste en importar la base de datos de temperatura mínima. El
archivo está en formato csv, por lo cual, es un fichero de texto plano. 
Se pueden usar varias funciones para realizar la importación. 
Se emplara´na los paquetes del `tidyverse` para realizar 
todo el tratamiento de datos.



::: {.exercise #ex2 name="Importación de los datos" }

Importe los datos `tempmin.csv` y guárdelos en un objeto llamado `tmin`.
:::

::: {.solution #sol2}
```{r tmin_import}
#Cada uno debe seleccionar el directorio donde tiene los datos, de ahí
#que sea conveniente trabajar con proyectos.
library(readr)
tmin <- read_csv("data/tempmin.csv")
```
:::


Los el conjunto de datos `tempmin` es un data frame que 
contiene 5 variables:

-   `fecha`: Indicando la fecha de observación.

-   `indicativo`: Es el identificador de la estación de la AEMET que registró el
    dato.

-   `tmin`: Dato de temperatura mínima registrada en cada fecha por la estación
    correspondiente en grados centígrados.

-   `longitud, latitud`: Coordenadas geográficas de la estación



::: {.exercise #ex3 name="Descripción de los datos" }
Con la función `head()` describa, en forma de tabla, la información 
que contiene el objeto `tmin` y compruebe que se corresponde con la descrita.
:::


```{r tmin-head}
knitr::kable(head(tmin), caption = "Detalle del objeto tmin")
```



Una de las clases de objetos espaciales más utilzada en R es `sf`. Sin embargo,
dependiendo del análisis que se quiera realizar hay otras muy comunes como 
`geodata`, `spatastat`, etc..

A continuación se convertirá el objeto `tmin` (`data.frame`) 
a un objeto de la clase `geodata`, una clase muy utilizada para trabajar con 
datos espacilaes y requerida por la librería `geoR`. 
Estos objetos contienen las coordenadas y la variable objeto de estudio.
Para mayor detalle ver `??as.geodata`. Obsérvese como varía la variable
a través de los ejes longitud y latitud. Observe tambien la forma campaniforme
de la distribución de la variale.


::: {.exercise #ex4 name="Descripción de los datos" }
Del objeto `tmin` seleccione el día **8 de enero de 2022** 
y las variables `longitud`, `latitud` y `tmin` para crear el objeto
y llámelo `tmin_geoR`. A continuación describa analítica y
gráficamente dicho objeto.
:::


```{r fig.cap="Convertir `data.frame` a `geodata`"}
library(dplyr)
library(geoR)

tmin_geoR <- tmin %>%
  filter(fecha == "2021-01-08") %>%
  # Seleccionamos las columnas de interés
  dplyr::select(longitud, latitud, tmin) %>%
  # Y creamos el objeto geodata
  as.geodata(
    coords.col = 1:2,
    data.col = 3
  )


summary(tmin_geoR)
plot(tmin_geoR)
```




En esta ocasión, convertiremos los datos de `tmin` en un objeto espacial `sf`, es
decir, datos espaciales de tipo vector.

Los datos de `tmin` contienen coordenadas geográficas longitud/latitud, así que
como se vió en la Sección **Sistema de Referencia de Coordenadas (CRS)** el CRS a
emplear ha de ser un CRS geográfico. Usaremos el código EPSG **4326**, que
corresponde a coordenadas geográficas y suele ser el habitual en este tipo de
situaciones.


::: {.exercise #ex5 name="Convertir `data.frame` a `sf`" }
Del objeto `tmin` seleccione las variables `longitud`, `latitud` y `tmin` 
para crear el objeto sf y llámelo `tmin_sf`. 
A continuación describa el objeto creado.
:::


```{r}
library(sf)

tmin_sf <- st_as_sf(tmin,
  coords = c("longitud", "latitud"),
  crs = 4326
)

tmin_sf
```



La siguiente tarea será representar las estaciones que monitorizan 
la temperatura mínima en un mapa de España. 

::: {.exercise #ex6 name="Reprsentación espacial de la clase sf" }
Represente un mapa de ESpaña, con las Comunidades Autónomas de incluidas,
excepto las Islas Canarias (por simplicidad)
:::

::: {.solution #sol6}
Una opción es utilizar un paquete API que nos proporciona esta información 
en formato `sf`, el paquete `mapSpain`.

```{r fig.cap="Mapa de España (Sin Canarias)"}
library(mapSpain)
# sf object
esp <- esp_get_ccaa() %>%
  # No vamos a usar Canarias en este análisis
  filter(ine.ccaa.name != "Canarias")


plot(esp$geometry) # Dibujamo el mapa de España menos las Islas Canarias
```
:::


Como se comentó en la sección **Sistema de Referencia de Coordenadas (CRS)**,
cuando se emplean datos geográficos provenientes de varias fuentes, es necesario
asegurarse de que ambos objetos están usando el mismo CRS.

::: {.exercise #ex7 name="CRS" }
¿Tengo el Sistema de referencia de coordenadas (CRS) de las estaciones de
monitoreo en la misma proyección que el contorno de España? Compruebelo. 
:::


```{r chek_crs}
st_crs(tmin_sf) == st_crs(esp)
```




Se ha comprobado que no lo están, por lo que hay que proyectar las coordenadas a un CRS
común. 

::: {.exercise #ex8 name="CRS" }
Proyecte las coordenadas de los objetos `tmin_sf` y `esp`
al CRS de referencia de `tmin_sf`. Compruébelo.
:::


```{r}
esp2 <- st_transform(esp, st_crs(tmin_sf))

st_crs(tmin_sf) == st_crs(esp2)
```



Para dibujar las estaciones de monitoreo con el contorno de España, existen 
varias opciones de paquetes, `ggplot2` (paquete de referencia en representaciones
gráficas),  `tmap` o `mapsf` (estos dos
últimos especializados en mapas temáticos,)


paquete `ggplot2` como referencia, sin embargo existen varios paquetes


::: {.exercise #ex9 name="CRS" }
Represente, con el paquete `ggplot2`, las estaciones de monitorero de AEMET
en la península Ibérica.
:::


```{r fig.cap="Estaciones de AEMET en la Península Ibérica"}
library(ggplot2)

ggplot(esp2) +
  # Para graficar objetos sf debemos usar geom_sf()
  geom_sf() +
  geom_sf(data = tmin_sf) +
  theme_light() +
  #labs(
  #  title = "Estaciones de monitoreo AEMET en  España",
  #  subtitle = "excluyendo las Islas Canarias"
  #) +
  theme(
    plot.title = element_text(
      size = 12,
      face = "bold"
    ),
    plot.subtitle = element_text(
      size = 8,
      face = "italic"
    )
  )
```


Una vez represtadas las coordenadas, es decir, las estaciones de monitoreo dónde se ha medido la variable
temperatura mínima `tmin`, el siguiente paso será representar el valor que toma 
la variable en esas coodenadas. La base `tmin` contine informaicón temporal 
para para varios días, por lo que, como este análisis es meramente espacial
se elegirá un día que se fijará para todo el análisis.


::: {.exercise #ex10 name="plot-sp1" }
Representamos la variable temperatura mínima `tmin` para el día **8 de
enero de 2021**. Gurarde la base de datos espacial para ese día en un objeto
de nombre `tmin_8enero`.
:::


```{r plot-base-tmin, fig.cap="Mapa de puntos con temperatura mínima (8-enero-2021)"}

# Seleccionaremos los datos correspondientes al 8 de enero de 2021 
tmin_8enero <- tmin_sf %>%
  filter(fecha == "2021-01-08")


# Mapa temático en el que se representan los valores de temperatura mínima 
# registrados en cada estación mediante un código de colores
plot(tmin_8enero["tmin"],
 # main = "Temperatura mínima (8-enero-2021)",
  pch = 8
)
```


El mapa ha quedado muy bien, pero quizá los colores y el formato elegido no sean los más
adecuados para este tipo de representaciones...

::: {.exercise #ex10 name="plot-sp2" }
Utilice los parámetros espaciales de los que dispone, las coordenadas y 
el contorno de España para graficar y contar la historia de **Filomena**
adecuadamente.
:::

```{r spatial-plots, echo=FALSE, fig.cap="Mapa completo con temperatura mínima (8-enero-2021)"}

# Especificamos la paleta de color a utilizar
cortes <- c(-Inf, seq(-20, 20, 2.5), Inf)
colores <- hcl.colors(15, "PuOr", rev = TRUE)


ggplot() +
  geom_sf(
    data = esp2,
    fill = "grey99"
  ) +
  geom_sf(
    data = tmin_8enero,
    aes(color = tmin),
    size = 4,
    alpha = .7
  ) +
  labs(color = "Temp. mín") +
  scale_color_gradientn(
    colours = colores,
    breaks = cortes,
    labels = function(x) {
      paste0(x, "º")
    },
    guide = "legend"
  ) +
  theme_light() +
  #labs(
  #  title = "Temperatura mínima (8-enero-2021)"
  #) +
  theme(
    plot.title = element_text(
      size = 12,
      face = "bold"
    ),
    plot.subtitle = element_text(
      size = 8,
      face = "italic"
    )
  )
```


La visión que ofrece el la Fig. \@ref{fig:spatial-plots} de Filomena es muy 
informativa, vemos como los datos nos cuentan la historia de lo que ocurrió 
ese día. La pena es que no existan estaciones de monitorero en todos los 
puntos de España para conocer el valor de la temperatura mínima en cualquier 
lugar del país. ¿Podríamos tener un mapa de interpolación para tener una estimación de la
temperatura mínima en las partes donde la AEMET no tiene estación de
monitoreo?


Tal y como se avanzó en el Capítulo \@ref(dep-esp), parece lógico pensar 
que aquellos puntos que estén cerca tendrán valores similares. Por tanto,
tomemos ventaja de las propiedades de la dependencia
espacial y utilicemos un método de interpolación sencillo, en este caso 
un método determinista, la Distancia Inversa
Ponderada, comúnmente conocido por su acrónimo inglés IDW (Inverse distance
weighted), el cual es uno de los métodos más simples para llevar para llevar a
cabo una interpolación espacial.


En este tipo de análisis espacial, es crucial que el CRS sea el apropiado. En este caso,
ya se definió el CRS como un CRS geográfico (es decir, usando coordenadas de
longitud y latitud). Sin embargo, para el ejercicio de interpolación es más
adecuado usar un CRS local (que provoca pocas deformaciones en la proyección de
España) y en alguna unidad de distancia, como metros (ya se vio en la Sección
XXXX que en los CRS geográficos las unidades son grados).


::: {.exercise #ex11 name="Obtención de CRS sugerido para un conjuto de datos" }
Utilice el paquete `crsuggest` para observar los CRS sugeridos y, si es necesario,
transforme la proyección de los datos.
:::

```{r}
library(crsuggest)

sugiere <- suggest_crs(tmin_8enero, units = "m", limit = 5)

# Usamos la sugerencia del paquete
crs_sugerido <- st_crs(sugiere[1, ]$crs_proj4) #Madrid

esp3 <- st_transform(esp2, crs_sugerido)
tmin_8enero3 <- st_transform(tmin_8enero, crs_sugerido)
```



Una vez solucionado el problema de las proyecciones, antes de llevar a cabo la
interpolación, es necesario generar una malla que representará
las celdas de las que queremos obtener el valor interpolado.
Dado que hemos proyectado nuestros datos a un CRS cuya unidad son los metros,
podemos definir el tamaño de cada celda en metros cuadrados. En este caso vamos
a usar celdas de 100 kms cuadrados (10 x 10 kms).



::: {.exercise #ex12 name="Creación y representación de una malla de interpolación" }
Genere un grid y llámelo `malla_sf` (puede fijar una semilla si lo desea) y 
grafíque la superficie construida.
:::


```{r create-grid2, fig.cap="Malla de puntos para interpolación"}

# Generación de la superficie a interpolar
set.seed(9876) # Aseguramos que el grid generado siempre es igual

malla_sf <- st_make_grid(
  esp3,
  cellsize = 8000
)

# Representación de la superficie construida añadiendo el contorno de España
ggplot(esp3) +
  geom_sf() +
  geom_sf(
    data = malla_sf,
    size = 0.1,
    col = "red", alpha = 1,
    fill = NA
  ) +
  geom_sf(
    data = tmin_8enero3,
    aes(fill = "AEMET Stations"), size = 4, shape = 21,
    color = "blue"
  ) +
  scale_fill_manual(values = adjustcolor("blue", alpha.f = 0.2)) +
  theme_void() +
  theme(legend.position = "bottom") 
```


Se puede observar claramente cada una de las celdas que se han creado. La
interpolación asignará un valor a cada uno de ellas.

A continuación podemos llevar a cabo la interpolación usando el paquete `gstat`.
Además, en lugar de celdas (polígonos) es necesario usar puntos en la
interpolación. Calcularemos, por tanto, un punto representativo de cada celda 
creada en la superficie anterior `malla_sf`, el centroide, 
que es el punto resultante de realizar la media arimética de las
coordenadas de los puntos que componen los lados de cada celda.


::: {.exercise #ex13 name="Interpolación a través de la Distancia Inversa Ponderada" }
Calcule los centroides de los polígonos de la malla construida 
en el Ejercicio \@ref(exr:ex12) con la función `st_centroide`
y realice una interpolación de la variable temperatura mínima 
`tmin` para el día 8 de enero de 2021 con el método IDW usando la librería
`gstat` y la función `idw`. Guarde el resultado obtenido en un objeto llamado
`tmin_idw`. Utilice la función `help(idw)` si requiere
información sobre cómo introducir los parámetros en la función.

Examine la información del objeto `tmin_idw`, a través de la función `head`, 
:::

```{r}
# Calculamos una malla con centroides
malla_sf_cent <- st_centroid(malla_sf, of_largest_polygon = TRUE)

library(gstat)
tmin_idw <- idw(
  # Indicamos la variable que queremos interpolar
  tmin ~ 1,
  # Indicamos el conjunto de datos donde está la variable
  tmin_8enero3,
  # Indicamos la malla de destino, en sf
  newdata = malla_sf_cent,
  idp = 2.0 # Especifica la potencia de la IDW
)
head(tmin_idw)
```


Un tipo de mapas muy utilizado cuando se trabaja con datos espaciales son los 
mapas de contorno. Es muy visual y ayuda a interpretar el mapa interpolado, 
añadir unas lineas de contorno al mapa interpolado.


::: {.exercise #ex14 name="Mapa de interpolación y contorno con `raster`" }
Represente los valores interpolados, `tmin_idw`, y añada unas lineas de contorno.
Utilice el paquete `raster` para convertir el objeto interpolado a pixeles.
:::

```{r, fig.cap="Mapa raster con lineas de nivel"}
# Convertimos de sf a SpatiaPixels
# Esto funciona porque nuestros puntos sf están espaciados regularmente

tmin_pixels <- tmin_idw %>%
  as("Spatial") %>%
  as("SpatialPixels")


library(raster)
# Creamos un raster de nuestros pixels
rast_esp <- raster(tmin_pixels)

# Transferimos valores del objeto sf al raster
rast_esp2 <- rasterize(
  tmin_idw,
  rast_esp,
  field = "var1.pred", ## valores de predicción idw
  fun = mean
)

# Además, podemos recortar el raster a la forma de España

rast_esp_mask <- mask(rast_esp2, esp3)

plot(rast_esp_mask, col = colores)
contour(rast_esp2, add = TRUE)
```



::: {.exercise #ex15 name="Mapa de interpolación con `ggplot`" }
Repita el mapa de \@ref(exr:ex14) usando `ggplot2` y la función
`geom_contour_filled`.
:::


```{r fig.cap="Temperatura mínima interpolada. 8 de Enero 2021. "}

# Creo una tabla para geom contour
coordenadas <- st_coordinates(tmin_idw)
valor <- tmin_idw$var1.pred

idw_df <- data.frame(
  # Necesitamos redondear las coordenadas
  latitud = round(coordenadas[, 2], 6),
  longitud = round(coordenadas[, 1], 6),
  tmin = valor
)

ggplot() +
  geom_contour_filled(
    data = idw_df,
    aes(x = longitud, y = latitud, z = tmin),
    na.rm = TRUE,
    breaks = cortes
  ) +
  # Reajustamos la escala de colores
  scale_fill_manual(values = colores) +
  # CCAA
  geom_sf(data = esp3, fill = NA) +
  theme_minimal() +
  theme(axis.title = element_blank()) +
  labs(
    fill = "Temp. (º)",
  #  title = "Temperatura mínima interpolada",
  #  subtitle = "8 de Enero 2021",
  #  caption = "Datos: AEMET"
  )
```






<!--  ------------------ Renta  -----------------  -->


## Caso 2. Distribución espacial de la renta media por municipios

** Objetivos de aprendizaje **

Esta sección presenta un caso de uso en el que aprenderemos a realizar las
siguientes tareas básicas:

-   Importar datos tabulares y datos espaciales.

-   Realizar un tratamiento de limpieza de datos y cruzar tablas.

-   Hacer mapas temáticos. Aprenderemos también algunas nociones básicas sobre
    cómo crear diferentes clases para un conjunto de datos continuo.

Para ello, partiremos de dos ficheros:

1.  Fichero `renta_municipio.csv`: Este fichero contiene información de la Renta
    Neta per cápita por municipios (en euros), distritos y secciones censales.
    Esta información se ha extraído del [Atlas de distribución de renta de los
    hogares](https://www.ine.es/experimental/atlas/experimental_atlas.htm)
    proporcionado por el INE, y ha sido tratado previamente para adaptar la
    información al presente ejercicio.

2.  Fichero `municipios.gpkg`: Es un fichero que contiene datos espaciales
    (polígonos) de los municipios en España en el año 2019. Se ha extraído del
    Instituto Geográfico Nacional (IGN) usando el paquete `mapSpain`.



El primer paso en cualquier tipo de análisis de datos es importar los datos al
software de tratamiento (en nuestro caso, R) y analizarlos para conocer el tipo
de información que contiene.

::: {.exercise #ex16 name="Importación y análisis del los datos objeto de estudio" }
Importe el fichero de datos `renta_municipio.csv` y `municipios.gpkg` 
y guárdelo en un objeto llamado `renta` y `munis`, respectivamente.
Observe la información que contienen. Puede ayudarse de la función `head.`
Use las librerías oportunas par iportar los datos en los distintos formatos.
:::

```{r importa_renta}
# Usaremos paquetes del tidyverse
library(dplyr)
library(readr)

renta <- read_csv("data/renta_municipio.csv", na = ".")
```

```{r importa_muni}
library(sf)
munis <- st_read("data/municipios.gpkg", quiet = TRUE)
```



```{r importa_renta_tabla}
head(renta)
```

Se puede comprobar que tenemos información para el periodo 2015-2019. Además, la
columna `Unidad` contiene un literal con el municipio o sección correspondiente.


```{r importa_munis_tabla}
head(munis)
```

El objeto `munis` contiene Polígonos y varias
columnas, entre ellas dos especialmente relevantes: `cpro` y `cmun`, que
corresponden a los códigos de provincia y de municipio respectivamente. Podemos
comprobar que este código también se encuentra en el dataset `renta`.


::: {.exercise #ex17 name="Comprobación de campos en común para un municipio: Noblejas" }
Para comrobar que efectivamente disponemos de dos campos en comun en los ficheros,
que serán de vital importancia para posteriormente unirlos, se selecciona un
municipio al azar, el municipio de Noblejas en la provincia de Toledo y 
comprobamos.
:::

```{r noblejas}
# Miro un municipio: Noblejas

renta[grep("Noblejas", renta$Unidad), ]

munis[grep("Noblejas", munis$name), c("name", "cpro", "cmun")]
```

En el caso de Noblejas, el código completo es 45115. Sin embargo, en el caso de
la tabla `renta`, debemos extraer ese valor del literal. Para ello debemos
manipular la columna y extraer la primera palabra de la columna `Unidad`:

```{r limpia-renta}

# Creo una función y la aplico a toda la columna
extrae_codigo <- function(x) {
  unlist(strsplit(x, " "))[1]
}

renta$codigo_ine <- sapply(as.character(renta$Unidad), extrae_codigo)

head(renta[c("Unidad", "codigo_ine")])
```

Ahora, es necesario crear la misma variable en `munis` para poder realizar el
cruce:

```{r codigo-ine}

munis$codigo_ine <- paste0(munis$cpro, munis$cmun)

head(munis[, c("name", "codigo_ine")])
```



Ya estamos listos para realizar el cruce. Además, seleccionaremos sólo las
columnas que vamos a usar, en este caso la del año 2019.


DIEGO, letfjoin está puesto sin guión porque da error en latex y no he sido capaz
de solucionarlo. Es un mal menor


::: {.exercise #ex18 name="Unión de objetos renta y mapas con la función `leftjoin`" }
Realice la unión de los objetos con la función `left_join` y seleccione las variables
`name`, `cpro`, `cmun`, `2019`. Guarde el resultado obtenido en un nuevo
objeto llamado `munis_renta`.
:::

```{r cruce_renta}

munis_renta <- munis %>%
  left_join(renta) %>%
  dplyr::select(name, cpro, cmun, `2019`)
```

**Cuando crucemos datos espaciales con datos no espaciales en R, es importante
que el primer dataset sea el que contiene los datos espaciales**. Esto es así
porque el objeto resultante "hereda" la clase del primer objeto.

¿Qué ocurre si realizáramos el proceso poniendo los datos espaciales en el
lado derecho del join? A modo de ejemplo, si realizáramos el proceso
poniendo los datos espaciales en
el lado derecho del join, los datos finales no serán espaciales:

```{r cruces_espaciales}

# Miramos la clase de munis_renta

class(munis_renta)

# Es un sf, por tanto espacial

# ¿Que pasa si realizamos el cruce de la otra manera?
renta %>%
  left_join(munis) %>%
  dplyr::select(name, cpro, cmun, `2019`) %>%
  class()
```

El resultado es un tibble o data.frame, **¡pero no es espacial!**

Una vez que tenemos los datos unidos podemos realizar algunos análisis básicos,
como la realización de un histograma.


::: {.exercise #ex19 name="Histograma de la variable Renta neta media por persona (€)" }
A través de un histograma represente la distribución de la variable Renta neta
media por persona del objeto `munis_renta` para el año 2019.
:::

```{r basic, fig.cap="Histograma de la variable Renta neta media por persona (€) en 2019"}

library(ggplot2)

munis_renta %>%
  ggplot(aes(x = `2019`)) +
  geom_histogram(color = "darkblue", fill = "lightblue") +
  scale_x_continuous(labels = scales::label_number_auto()) +
  scale_y_continuous(labels = scales::label_percent()) +
  labs(
    y = "",
    x = "Renta neta media por persona (€)"
  )
```


Se puede observar que la renta presenta una distribución Gamma con un gran
número de municipios concentrados en zonas medias de renta y pocos municipios en tramos de
rentas altas. Como se verá en el Ejercicio XXX, esta distribución va a afectar a la
información que transmite el mapa.


::: {.exercise #ex20 name=" Mapa de coropletas de la distribución de la renta" }
Realice un mapa de coropletas mostrando la distribución de la
renta usando los valores brutos de renta sin modificar
:::

```{r maparenta1, fig.cap="Renta neta media por persona en España (2019)" }

ggplot(munis_renta) +
  # Usamos geom_sf, y como aes() lo que queremos mostrar, en este caso, el
  # color del polígono representa la renta. Vamos a retirar los bordes con
  # color = NA
  geom_sf(aes(fill = `2019`), color = NA) +
  theme_minimal() +
  scale_fill_continuous(labels = scales::label_number(
    big.mark = ".",
    decimal.mark = ",",
    suffix = " €"
  ))# +
  # labs(
  #  title = "Renta neta media por persona",
  #  caption = "Datos: INE"
  # )
```

Este primer mapa no es demasiado informativo, por los siguientes motivos:

-   Existe una serie de municipios para los que no tenemos datos.

-   La escala de color no es la más adecuada.

-   Dada la distribución de los datos, puede ser adecuado crear grupos de renta
    para que el mapa sea más interpretable.



::: {.exercise #ex21 name="Adecuacion de los datos al la visualización" }
Elimine los municipios sin datos y a cambie la escala de
color para ver si mejora la visualización de la variable Renta neta media por persona.
:::


```{r maparenta2, fig.cap="Renta neta media por persona en España (2019)" }

munis_renta_clean <- munis_renta %>% filter(!is.na(`2019`))

ggplot(munis_renta_clean) +
  geom_sf(aes(fill = `2019`), color = NA) +
  # Cambiamos la paleta de colores, vamos a usar una paleta denominada Inferno,
  # ya incluida en base R con hcl.colors

  # Como son datos continuos, puedo usar Inferno
  scale_fill_gradientn(
    colours = hcl.colors(20, "Inferno", rev = TRUE),
    labels = scales::label_number(
      big.mark = ".",
      decimal.mark = ",",
      suffix = " €"
    )
  ) +
  theme_minimal()
 # + labs(
 #    title = "Renta neta media por persona",
 #    caption = "Datos: INE"
 #  )
```

Este mapa proporciona algo más de información, y parece intuirse que las rentas más
altas se encuentran en zonas de País Vasco, Madrid y Cataluña. Sin embargo, el
hecho de que la distribución de los datos no sea normal está afectando a la
visualización.

Para intentar atajar este problema, se puede dividir los datos en clases,
por ejemplo, cuartiles o deciles. Existen varios métodos de clasificación de
datos, que en R se encuentran implementados en el paquete `classInt`. A
continuación se van a plantear diversos métodos de clasificación y se
observará cómo la "historia" que cuenta el mapa varía en función de dichas
clases. Se proponen siguientes métodos de clasificación:

-   **El método de deciles**: consiste en crear 10 categorías incluyendo el mismo
    número de registros en cada una de ellas.

-   **El método de intervalos equivalentes**: divide el rango de valores en un
    número de grupos definido. La distancia de todos los intervalos es idéntica,
    por lo que este método no tiene en cuenta la distribución de los registros.

-   **El método de Fisher-Jenks**: desarrollado específicamente para la
    clasificación de datos espaciales y su visualización en mapas. Produce
    agrupaciones de tal manera que los datos de cada grupo son "cercanas" entre
    sí y sustancialmente distintas de los valores de otros grupos.


::: {.exercise #ex22 name="División de los datos en clases" }
Utilice la función `classIntervals` del paquete `classInt`
y cambie el parámetro `style` para obtener los métodos de
clasificación: Deciles, tramos de Renta equidistantes y Fisher and Jenks.

Realice un `plot` de las clases obtenidas y compárelas.
:::

```{r classint, message=FALSE}

library(classInt)

# División en deciles
deciles <- classIntervals(munis_renta_clean$`2019`,
  style = "quantile", n = 10
)
deciles
plot(deciles, pal = hcl.colors(20, "Inferno"), main = "Deciles")


# Tramos equidistantes en términos de renta
equal <- classIntervals(munis_renta_clean$`2019`,
  style = "equal", n = 10
)
equal
plot(equal, pal = hcl.colors(20, "Inferno"), main = "Equidistantes")

fisher <- classIntervals(munis_renta_clean$`2019`,
  style = "fisher",
  # Fuerzo para mejorar la comparación entre métodos
  n = 10
)
fisher
plot(fisher,
  pal = hcl.colors(20, "Inferno"),
  main = "Fisher-Jenks"
)
```

Se puede observar lo siguiente:

-   El último decil de renta se corresponde a un rango de entre 15.000 y 25.000
    €.

-   El método por deciles proporciona unos grupos con valores de renta muy
    parecidos entre sí en los valores medios. Esto es debido a la propia
    distribución de la variable.

-   El método de rangos equidistantes proporciona algunos grupos con un número
    muy reducido de municipios.

-   El método de Fisher-Jenks puede proporcionar unas clases con unos rangos más
    apropiados para los tramos altos de renta.




::: {.exercise #ex23 name="Representación de los mapas según las clases obtenidas" }
Realice tres mapas distintos, creando clases de renta según cada uno
de los métodos anteriormente mostrados y coméntelos.
:::

**Deciles**

```{r mapa-deciles, message=FALSE, fig.cap="Mapa por deciles de renta media por persona (2019)"}
# Extracción de los valores de corte
breaks_d <- deciles$brks

# Creación de etiquetas básicas para cada clase
# Creación de una función específica para crear etiquetas formateadas
label_fun <- function(x) {
  l <- length(x)
  eur <- paste0(prettyNum(round(x, 0),
    decimal.mark = ",",
    big.mark = "."
  ), " €")

  labels <- paste(eur[-l], "-", eur[-1])
  labels[1] <- paste("<", eur[1])
  labels[l - 1] <- paste(">", eur[l - 1])
  return(labels)
}

labels_d <- label_fun(breaks_d)

munis_renta_clean$Deciles <- cut(munis_renta_clean$`2019`,
  breaks = breaks_d,
  labels = labels_d,
  include.lowest = TRUE
)

ggplot(munis_renta_clean) +
  # Cambio la variable a representar para crear el mapa
  geom_sf(aes(fill = Deciles), color = NA) +
  # Cambio el scale, ya no es continua
  scale_fill_manual(values = hcl.colors(length(labels_d),
    "Inferno",
    rev = TRUE
  )) +
  theme_minimal() # +
  #labs(
  #  title = "Renta neta media por persona",
  #  caption = "Datos: INE"
  # )
```

El mapa de la Fig. \@ref(fig:mapa-deciles) ya permite observar patrones
geográficos, donde se ve una clara diferencia entre la Comunidades Autónomas del
Norte y las del Sur. Veamos una representación distina usando otras clases
diferentes

```{r mapa-equal, message=FALSE, fig.cap="Mapa por tramos de renta equidistantes de renta media por persona (2019)"}

breaks_e <- equal$brks
labels_e <- label_fun(breaks_e)

munis_renta_clean$Equal <- cut(munis_renta_clean$`2019`,
  breaks = breaks_e,
  labels = labels_e,
  include.lowest = TRUE
)

ggplot(munis_renta_clean) +
  # Cambiamos la variable que usamos para crear el mapa
  geom_sf(aes(fill = Equal), color = NA) +
  scale_fill_manual(values = hcl.colors(length(labels_e),
    "Inferno",
    rev = TRUE
  )) +
  theme_minimal() # +
  # labs(
  #  title = "Renta neta media por persona",
  #  caption = "Datos: INE"
  #)
```

El mapa de la Fig. \@ref(fig:mapa-deciles), sin embargo, se parece más al mapa
de la Fig. \@ref(fig:maparenta2) con los datos sin clasificar, donde el peso visual se concentra más
bien en los municipios con rentas mucho más altas que el resto (por encima de
18.000 €).

Véase a continuación el mismo mapa usando la clasificación Fisher-Jenks:

```{r mapa-fisher, message=FALSE, fig.cap="Mapa por tramos según Fisher-Jenks"}

breaks_f <- fisher$brks
labels_f <- label_fun(breaks_f)

munis_renta_clean$`Fisher-Jenks` <- cut(munis_renta_clean$`2019`,
  breaks = breaks_f,
  labels = labels_f,
  include.lowest = TRUE
)

ggplot(munis_renta_clean) +
  # Cambiamos la variable que usamos para crear el mapa
  geom_sf(aes(fill = `Fisher-Jenks`), color = NA) +
  scale_fill_manual(values = hcl.colors(length(labels_f),
    "Inferno",
    rev = TRUE
  )) +
  theme_minimal() +
  labs(
    title = "Renta neta media por persona",
    caption = "Datos: INE"
  )
```

En el mapa de la Fig. \@ref(fig:mapa-deciles) se puede observar de una manera
más clara un cluster adicional de renta en la zona de Asturias y el norte de
León. Además, gracias a la escala de colores puede intuirse que este clúster de
renta no presenta valores tan altos como los observados en País Vasco o Madrid.

En conclusión, en el momento de realizar una visualización de datos es
importante conocer el dato a representar, así como entender algunas propiedades
básicas de la distribución subyacente. También se ha podido observar que hay
ciertas decisiones estéticas (datos continuos vs. agrupados, escala de colores)
que tienen una influencia significativa en cómo se percibe la información
representada. Es responsabilidad del investigado encargado de
crear de la visualización el conocer
todos estos factores y aplicarlos de manera conveniente.




<!-- ::: {.exercise #ex24 name="Representación de la información sesgada" } -->
<!-- Pra finalizar este caso práctico, se propone al lector la siguiente cuestión. -->
<!-- El mapa de la Fig. \@ref{fig:trucado} se ha realizado con el mismo  -->
<!-- conjunto de datos, sin embargo la información se presenta de -->
<!-- manera sesgada, ¿puedes identificar los motivos? -->
<!-- ::: -->

<!-- ```{r trucado, echo=FALSE, fig.cap="Ejemplo de visualización sesgada"} -->

<!-- breaks_s <- c(seq(5500, 12000, 500), 90000) -->
<!-- labels_s <- label_fun(breaks_s) -->

<!-- munis_renta_clean$Sesgada <- cut(munis_renta_clean$`2019`, -->
<!--   breaks = breaks_s, -->
<!--   labels = labels_s, -->
<!--   include.lowest = TRUE -->
<!-- ) -->

<!-- ggplot(munis_renta_clean) + -->
<!--   geom_sf(aes(fill = Sesgada), color = NA) + -->
<!--   scale_fill_manual(values = c(hcl.colors(20, -->
<!--     "Blue-Red2", -->
<!--     rev = FALSE -->
<!--   )[1:12], hcl.colors(10, "Reds2", rev = TRUE)[8:10])) + -->
<!--   theme_minimal()  -->
<!-- ``` -->

<!-- En el mapa de la Fig. \@ref(fig:trucado) parece que la renta per cápita de las -->
<!-- comunidades del norte es desproporcionadamente superior a las del sur. Un resumen -->
<!-- de los sesgos introducidos en el mapa se concreta en: -->

<!-- -   En primer lugar, se han creado un elevado número de grupos en las zonas de -->
<!--     rentas bajas. De esta manera, la escala del mapa parece estar muy -->
<!--     fragmentada, sin embargo muchos de esos grupos apenas contienen municipios. -->
<!--     A modo de ejemplo, los primeros cuatro grupos únicamente contienen 32 -->
<!--     municipios. -->

<!-- -   Los grupos no se adaptan a la distribución subyacente de los datos. La -->
<!--     mediana de los datos (11.462 €) estaría situada en la antepenúltima clase, -->
<!--     de manera que los dos grupos de mayor renta contienen el 50% de los -->
<!--     municipios. -->

<!-- -   Además, la escala de color se ha manipulado, de manera que los grupos de -->
<!--     mayores renta destaquen más que el resto de manera notoria. -->




<!-- ...............  patrones de puntos ............  -->


## Caso 3. Distribución espacial de los delitos cometidos en la ciudad de Valencia en el año 2010.

Las técnicas de análisis de patrones de puntos analizan la distribución de
eventos geolocalizados que surgen al azar. La diferencia fundamental con otros
análisis que comprenden también el uso de localizaciones (como las temperaturas
mínimas medidas por estaciones meteorológicas) es que en este caso los puntos
representan eventos conocidos y aleatorios (por ejemplo, los delitos ocurridos
en una ciudad, accidentes de tráfico o incendios en una región). A diferencia de
otros eventos, como el ejemplo de las temperaturas mínimas, la ausencia de datos
no se debe a la ausencia de medición (es decir, no existe una estación meteorológica
en ese lugar), si no a que no se ha producido el evento en dicha localización.

**Objetivo de aprendizaje**:

El alumno debe ser capaz de conocer los datos de tipo patrones de punto,
identificarlos y representarlos adecuadamente.

-  describir los ficheros. Hay que generar uno que sólo tenga 2010 y los datos
que utilizamos. lo llamamos igual y vale todo



::: {.exercise #ex24 name="Representación de la información sesgada" }
El presente análisis se va a realizar empleando RStudio, por lo que empezaremos
abriendo el programa y creando un nuevo script de R en *Proyecto>File>New File>R
script*.
:::

** habría que poner esto al inico de las 3 prácticas. comprobar.


*Tarea 2: Importamos y describimos los datos objeto de estudio*

El primer paso consiste en importar la base de datos de crímenes en la ciudad de
Valencia. El archivo está en formato csv, por lo que usaremos el paquete `readr`
para importar los datos:

```{r}
library(readr)
library(dplyr)

# En este caso el archivo está en la carpeta "data" de nuestro proyecto
crimen <- read_csv("data/crime-data-Valencia.csv")


summary(crimen)
```

**¿Qué información contiene los datos que se van a analizar?** Este archivo contiene en total
`r prettyNum(nrow(crimen), big.mark = ".", decimal.mark = ",")` registros, y
proporciona `r ncol(crimen)` campos asociados a cada registro.

Entre los campos disponibles, destacamos los campos `crime_lon` y `crime_lat`:
Son las coordenadas en las que se produjo el crimen.




::: {.exercise #ex26 name="CRS de las coordenadas" }
Determine el CRS en el que se encuentras las coordenadas del conjuto de datos
`crimen`, etiquetas con `crime_lon` y `crime_lat`
:::

```{r coods-lonlat, echo=FALSE}
crimen %>%
  head() %>%
  dplyr::select(crime_lon, crime_lat) %>%
  knitr::kable(caption = "Crímenes en Valencia; Coordenadas")
```


Obsérvese que, las coordenadas parecen corresponder con longitudes y latitudes,
ya que como se explicó el rango posible de valores es $[-180, 180]$ (para
longitudes) y $[-90, 90]$ (para latitudes):


::: {.exercise #ex27 name="Proyección y representación de localizaciones" }
Convierta el objeto `crimen` a un objecto `sf` llamado `crimen_sf`, teniendo en
cuenta el sistema CRS más adecuado. 
A modo de recordatorio, el CRS correspondiente a coordenadas
geográficas longitud/latitud es **EPSG:4326**.

Una vez proyectdas las coordenadas represéntelas en un mapa. Si lo desea
puede usar una imagen de fondo con la función `esp_getTiles` de la librería
`mapSpain`.
:::


```{r crimen1, fig.cap="Crímenes en Valencia"}

library(sf)
# Objeto sf sin CRS

crimen_sf <- st_as_sf(
  crimen,
  coords = c(
    "crime_lon",
    "crime_lat"
  ),
  crs = st_crs(4326)
)

# Comprobamos con un mapa base

library(mapSpain)
library(ggplot2)

# Usamos imagen como mapa de fondo
tile <- esp_getTiles(crimen_sf, "IDErioja",
  zoommin = 1,
  crop = TRUE
)

ggplot() +
  layer_spatraster(tile) +
  geom_sf(
    data = crimen_sf,
    col = "blue",
    size = 0.3,
    alpha = 0.3
  )
```

¿Hay algún patrón en la ocurrencia de crímenes?**

En la Fig. \@ref(fig:crimen1) podemos intuir ciertos patrones en la ocurrencia
de crímenes. Por ejemplo, parecen concentrarse en zonas céntricas y no hay
crímenes registrados en la zona del puerto.

Para el siguiente análisis, vamos a analizar el patrón de crímenes del año 2010.

** diego, engancha esto con lo anterior, porque sólo vamos a usar un año, y así 
evitamos problemas




```{r crimen2, fig.cap="Crímenes en Valencia (2010)"}
crimen_2010_sf <- crimen_sf %>%
  filter(
    year == "2010"
  )

ggplot() +
  layer_spatraster(tile) +
  geom_sf(
    data = crimen_2010_sf,
    col = "blue",
    size = 0.3,
    alpha = 0.3
  )
```


 *


El paquete `spatstat` (@spatstat_2005) es el paquete de referencia cuando se
trabaja con patrones de puntos

Siguiendo el anterior ejemplo, vamos a analizar el patrón de crímenes en el año
2010. Además, en el análisis de patrones de puntos es necesario delimitar la
ventana espacial de observación (owin). En este caso será el municipio de
Valencia.

::: {.exercise #ex37 name="Análisis de patrones con `spatstat`" }
FALTA
:::

```{r objeto_ppp}

# Extraigo Valencia con mapSpain

valencia <- esp_get_munic(munic = "^Valencia$") %>%
  # Necesito proyectar, en este caso usamos ETRS89-UTM huso 30 EPSG:25830
  st_transform(25830)



library(spatstat)
# Necesitamos un recinto de observación: owin

val_owin <- as.owin(valencia)

# Extraemos las coordenadas de los crímenes. Han de estar en el mismo CRS
# que el owin

coords <- crimen_2010_sf %>%
  st_transform(25830) %>%
  st_coordinates()

mydata_ppp <- ppp(
  x = as.numeric(coords[, 1]),
  y = as.numeric(coords[, 2]),
  window = val_owin
)

plot(mydata_ppp)
```


ERROR EN EL NOMBRE mydata_ppp, es por el guión.

::: {.exercise #ex38 name="Información del objeto `mydatappp`" }
FALTA
:::

¿Qué información contiene nuestro objeto en formato ppp?

```{r}
summary(mydata_ppp)
```

*Tarea 4: .*

Es importante determinar si los puntos se distribuyen al azar o tienen algún
patrón. Por ello, lo primero que haremos será representar el objeto `feb_ppp` y
superponer unos cuadrantes para su comportamiento (véase Fig.
\@ref(fig:cuadrante)).


::: {.exercise #ex37 name="Cálculo de la densidad de los delitos en Valencia en 2010 mediante cuadrantes" }
FALTA
:::


** ESte mapa se puede poner un poco más mono?? Yo ya lo he arregado un poco

```{r cuadrante, fig.cap="Crímenes en Valencia por cuadrantes (2010)"}
## Hallamos los cuadrantes
cuadrante <- quadratcount(mydata_ppp,
  nx = 5,
  ny = 5
)

## Dibujamos el número de crímenes que hay en cada cuadrante
plot(mydata_ppp, pch = "+", main = "", cex = 0.3)
plot(cuadrante, add = TRUE, col = "red", cex = 0.6, lwd=2)
```

Como se puede apreciar en la Fig. \@ref(fig:cuadrante) hay cuadrantes que
registran cero crímenes y otros que registran hasta 2.860 crímenes.


::: {.exercise #ex37 name="Estimación de la densidad de los delitos en Valencia en 2010 mediante de patrones de puntos" }
FALTA

density {stats}

:::


```{r intensidad-ppp, fig.cap="Estimación kernel de la intensidad de crímenes en la ciudad de Valencia " }

#Permite ver la variación local de la intensidad, graficando la estimación kernel de la intensidad.
densidad <- density(mydata_ppp) 

plot(densidad, main = " ")
points(mydata_ppp, pch = "+", cex = 0.5)
```

En la Fig. \@ref(fig:intensidad-ppp) se muestra la estimación kernel
de la densidad de patrones de puntos Los conocimientos teóricos necesarios para llevar a cabo
este tipo de estimación superan el ámbito de este manual. El objetivo
de este ejemplo es meramente ilustrativo.


